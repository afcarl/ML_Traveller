{
 "metadata": {
  "name": "",
  "signature": "sha256:64dab836d8de2ce2e8e5581ba02b350f210f2e87f289619e1a5ca224418b3ff4"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# The Missing Characters\n",
      "\n",
      "For the NLP task I decided to solve the [The Missing Characters](https://www.hackerrank.com/challenges/the-missing-characters) problem. My solution is currently ranked sixth on the leaderboard.\n",
      "\n",
      "\n",
      "## Trie data structure\n",
      "\n",
      "I solved the problem by using a [trie](http://en.wikipedia.org/wiki/Trie) data data structure. A trie is a search tree that can efficiently store a set of words, and the number they occurred in the text corpus. Each node in the trie contains a hashmap that stores the character continuations at the current position in the word. For example, the first node stores all possible characters that occur in the beginning of the words in the corpus. A node at level n has already been lead by n-1 previous characters, and matches the n-th character in the word. The wikipedia page gives a nice illustration. This trie can then be used to mach words with missing letter via the `get_trie_count_missing` function we defined.\n",
      "\n",
      "\n",
      "## Offline training\n",
      "\n",
      "The task mentionned that [this corpus of text](http://hr-testcases.s3.amazonaws.com/1307/assets/corpus.txt) can be used for offline training. What this Python notebook does is to build a trie model out of this corpus and compresses it into a string format so it can be integrated in the final solution file. During execution of the test this trie is extracted from the compressed string, and updated with any other words that are provided during testing.\n",
      "\n",
      "NLTK's [RegexpTokenizer](http://www.nltk.org/api/nltk.tokenize.html#module-nltk.tokenize.regexp) is used to tokenize the text, and Scikit-learn's [CountVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) is used to count all the words. We can't store all words in the trie since the hackerrank challenge only accepts files with a maximum size of 50KB. This is why only the most occuring words are extracted to build a trie that is not to big to store in text format. zlib is used to compress the dictionary, and base64 is used to convert this compression into a string that can be pasted into the final solution. The final solution file is made out of the functions defined in this notebook.\n",
      "\n",
      "\n",
      "## Finding missing letters\n",
      "\n",
      "During testing the trie is used to match words with missing letters. The resulting words are compared to the word with missing letters and the missing letters are filled in. If no match can be made the letter `e` is returned since this is the [most common letter in English](http://en.wikipedia.org/wiki/Letter_frequency).\n",
      "\n",
      "\n",
      "## Possible improvements\n",
      "\n",
      "If we can store more words in the trie during preprocessing we might get better results, a better compression schema might allow to insert more words. Also using a python trie that is optimised so that it can contain strings of character might reduce the size of the compressed trie.\n",
      "\n",
      "In the current solution if no match can be made the letter `e` is returned. It might be possible to improve on this heuristic, or define a simple local prediction model that predicts the missing character based on the surrounding characters.\n",
      "\n",
      "What would also be interesting is took look at Ilya Sutskever's recurrent neural network model described in ['Generating text with recurrent neural networks'](http://www.cs.utoronto.ca/~ilya/pubs/2011/LANG-RNN.pdf). This model however is not trivial to implement and train, Sutskever reports a training time of five days on 8 high-end GPUs."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# https://www.hackerrank.com/challenges/the-missing-characters\n",
      "import nltk\n",
      "import string\n",
      "import re\n",
      "from nltk.tokenize import RegexpTokenizer\n",
      "import urllib2  # the lib that handles the url stuff\n",
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "import zlib\n",
      "import base64\n",
      "import ast\n",
      "import os"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Define a regex tokenizer to extract all words made up of letter (also unicode)\n",
      "# and that can contain ', -, or \u2019 between 2 characters\n",
      "all_tokenize_regex = r'(\\s|--|[^\\w#]|\\d|_)+'\n",
      "word_tokenizer = RegexpTokenizer(all_tokenize_regex, gaps=True, flags=re.UNICODE)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# open file\n",
      "text_file = urllib2.urlopen('http://hr-testcases.s3.amazonaws.com/1307/assets/corpus.txt')\n",
      "# http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer\n",
      "# Countvectorizer that extracts and counts all the words with the help of word_tokenizer\n",
      "vectorizer = CountVectorizer(\n",
      "    input=u'file',\n",
      "    tokenizer=word_tokenizer.tokenize, \n",
      "    encoding=u'utf-8',\n",
      "    lowercase=True,\n",
      "    max_features=8200\n",
      ")\n",
      "# extract the words from the example text file\n",
      "counts = vectorizer.fit_transform([text_file])\n",
      "words = vectorizer.get_feature_names()\n",
      "print \"nb of extracted words: \", len(words)\n",
      "\n",
      "# create a dictionary with word:count\n",
      "count_dict = dict()\n",
      "for idx, c in enumerate(words):\n",
      "    count_dict[words[idx]] = counts[0,idx]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "nb of extracted words:  8200\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Create a trie to store the words\n",
      "# The end of a trie branch is delimited with the empty string ''\n",
      "\n",
      "def make_trie(word_dict):\n",
      "    \"\"\"\n",
      "    Function to create a trie from a dictionary with word:count.\n",
      "    Return a trie.\n",
      "    \"\"\"\n",
      "    root = dict()\n",
      "    # Add each word in the word dictionary\n",
      "    for word, count in word_dict.iteritems():\n",
      "        current_dict = root\n",
      "        # add each letter in the word\n",
      "        for letter in word:\n",
      "            current_dict = current_dict.setdefault(letter, {})\n",
      "        current_dict.setdefault('', count)\n",
      "    return root\n",
      "\n",
      "def update_trie(word, trie):\n",
      "    \"\"\"\n",
      "    Function to update the given trie with the given word.\n",
      "    \"\"\"\n",
      "    current_dict = trie\n",
      "    for letter in word:\n",
      "        current_dict = current_dict.setdefault(letter, {})\n",
      "    current_dict.setdefault('', 0)\n",
      "    current_dict[''] += 1\n",
      "\n",
      "def get_trie_count_missing(word, trie):\n",
      "    \"\"\"\n",
      "    Function to try to find a word in the given try, \n",
      "    taken into account that the word can have missing letters (#).\n",
      "    \"\"\"\n",
      "    current_trie_list = [('', trie)] # List that holds all matches\n",
      "    for letter in word:\n",
      "        if letter == '#':\n",
      "            # Add all possible matches\n",
      "            current_trie_list = [(s+char, trie[char]) \n",
      "                                 for char in string.lowercase \n",
      "                                 for (s, trie) in current_trie_list \n",
      "                                 if char in trie]\n",
      "        else:\n",
      "            # Update all matches \n",
      "            current_trie_list = [(s+letter, trie[letter]) \n",
      "                                 for (s, trie) in current_trie_list \n",
      "                                 if letter in trie]\n",
      "    # Aggregate all found words an counts\n",
      "    results = [(s, trie['']) for (s, trie) in current_trie_list if '' in trie]\n",
      "    # Sort the aggreagation by counts\n",
      "    results.sort(reverse=True, key=lambda tup: tup[1])\n",
      "    # Return None if no matches are found\n",
      "    if len(results) == 0:\n",
      "        return (None, 0)\n",
      "    # Return the best matching word if matches are found\n",
      "    return results[0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Create a trie and test it\n",
      "trie = make_trie(count_dict)\n",
      "\n",
      "print 'then match: ', get_trie_count_missing('then', trie)\n",
      "print 'th#n match: ', get_trie_count_missing('th#n', trie)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "then match:  ('then', 1034)\n",
        "th#n match:  ('then', 1034)\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Create a compressed serialization of the trie and store as a file\n",
      "\n",
      "file_name = 'trie.txt'\n",
      "\n",
      "# Compress the file\n",
      "serialized = base64.encodestring(zlib.compress(str(trie), 9))\n",
      "with open(file_name, 'w') as f:\n",
      "    f.write(serialized)\n",
      "\n",
      "# Open and test the serialization\n",
      "with open(file_name, 'r') as f:\n",
      "    serialized = f.read()\n",
      "    trie = ast.literal_eval(zlib.decompress(base64.decodestring(serialized)))\n",
      "\n",
      "# Check the filesize and if the trie was reconstructed correctly\n",
      "print 'filesize: ', os.path.getsize(file_name)\n",
      "print 'then match: ', get_trie_count_missing('then', trie)\n",
      "print 'th#n match: ', get_trie_count_missing('th#n', trie)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "filesize:  47060\n",
        "then match:  ('then', 1034)\n",
        "th#n match:  ('then', 1034)\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Create an example text with missing letters\n",
      "\n",
      "missing_text = u\"\"\"\n",
      "The Port#guese were present in some \u2013 mostly c#astal \u2013 points of the territory \n",
      "of what i# now Angola, from the 16th to the 19th ce#tury, inter#cting in diverse \n",
      "wa#s wi#h the peoples who lived there. In the 19th century, they slowly and \n",
      "hesi#antl# bega# to #stablish themselves in the int##ior. Angola as a Portu#uese \n",
      "colony encompassing #he pre#ent terri#ory was not establishe# #efore the end of \n",
      "the 19th century, and \"effective occupation\", #s required by the Berlin Conference \n",
      "(1884) was achieved only by the 1920s after the Mbunda resistance and abduction of \n",
      "the#r King, Mwene Mbandu I #yondthzi Kapov#. Independen#e was achieved in 1975, \n",
      "after a protracted liberation war. After independence, Angola was t#e scene of a# \n",
      "in#ense civil war from 1975 to 2002. Despite the civil war, areas such as Baixa de \n",
      "Cassanje continue a lineage of kings which have #ncluded the former #ing Kambamba \n",
      "Kulaxingo and c#rrent King Dianhenga Aspirante Mjinji Kulaxingo. #.\n",
      "\"\"\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Update the trie and test if there is a difference\n",
      "print get_trie_count_missing('the', trie)\n",
      "# get the tokens\n",
      "all_tokens = word_tokenizer.tokenize(missing_text)\n",
      "all_tokens = [token.lower() for token in all_tokens]\n",
      "# update the trie\n",
      "for token in all_tokens:\n",
      "    update_trie(token, trie)\n",
      "print get_trie_count_missing('the', trie)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "('the', 36152)\n",
        "('the', 36166)\n"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Define an extractor to extract the words with missing letters\n",
      "list_of_missing = [token for token in all_tokens if '#' in token]\n",
      "\n",
      "# print a list of words with missing values\n",
      "print list_of_missing"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[u'port#guese', u'c#astal', u'i#', u'ce#tury', u'inter#cting', u'wa#s', u'wi#h', u'hesi#antl#', u'bega#', u'#stablish', u'int##ior', u'portu#uese', u'#he', u'pre#ent', u'terri#ory', u'establishe#', u'#efore', u'#s', u'the#r', u'#yondthzi', u'kapov#', u'independen#e', u't#e', u'a#', u'in#ense', u'#ncluded', u'#ing', u'c#rrent', u'#']\n"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# define a function that returns the missing letters\n",
      "def get_missing_letters(trie, word):\n",
      "    \"\"\"\n",
      "    Function that tries to match the word with missing value on the trie\n",
      "    and return the missing letters.\n",
      "    \"\"\"\n",
      "    result = get_trie_count_missing(word, trie)\n",
      "    # Find all the indices\n",
      "    idxs = [i.start() for i in re.finditer('#', word)]\n",
      "    # match the indicies with the result\n",
      "    # if no match is found, return 'e', the most common letter in English\n",
      "    return (result, [(result[0][i] if result[0] is not None else 'e') for i in idxs])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Print all the missing words, their results and letters found:\n",
      "\n",
      "for word in list_of_missing:\n",
      "    result, letters = get_missing_letters(trie, word)\n",
      "    # print 'e' if a '#' was returned\n",
      "    print word, result[0], [(l if l is not '#' else 'e') for l in letters]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "port#guese portuguese [u'u']\n",
        "c#astal None ['e']\n",
        "i# in [u'n']\n",
        "ce#tury century [u'n']\n",
        "inter#cting None ['e']\n",
        "wa#s ways [u'y']\n",
        "wi#h with [u't']\n",
        "hesi#antl# None ['e', 'e']\n",
        "bega# began [u'n']\n",
        "#stablish establish [u'e']\n",
        "int##ior interior [u'e', u'r']\n",
        "portu#uese portuguese [u'g']\n",
        "#he the [u't']\n",
        "pre#ent present [u's']\n",
        "terri#ory territory [u't']\n",
        "establishe# established [u'd']\n",
        "#efore before [u'b']\n",
        "#s as [u'a']\n",
        "the#r their [u'i']\n",
        "#yondthzi None ['e']\n",
        "kapov# None ['e']\n",
        "independen#e independence [u'c']\n",
        "t#e the [u'h']\n",
        "a# at [u't']\n",
        "in#ense intense [u't']\n",
        "#ncluded included [u'i']\n",
        "#ing king [u'k']\n",
        "c#rrent current [u'u']\n",
        "# a ['a']\n"
       ]
      }
     ],
     "prompt_number": 11
    }
   ],
   "metadata": {}
  }
 ]
}